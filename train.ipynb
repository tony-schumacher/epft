{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epftoolbox.models import evaluate_dnn_in_test_dataset, hyperparameter_optimizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of layers in DNN\n",
    "nlayers = 2\n",
    "\n",
    "# Market under study. If it not one of the standard ones, the file name\n",
    "# has to be provided, where the file has to be a csv file\n",
    "dataset = 'DE'\n",
    "\n",
    "# Number of years (a year is 364 days) in the test dataset.\n",
    "years_test = 2\n",
    "\n",
    "# Optional parameters for selecting the test dataset, if either of them is not provided, \n",
    "# the test dataset is built using the years_test parameter. They should either be one of\n",
    "# the date formats existing in python or a string with the following format\n",
    "# \"%d/%m/%Y %H:%M\"\n",
    "begin_test_date = None\n",
    "end_test_date = None\n",
    "\n",
    "# Boolean that selects whether the validation and training datasets are shuffled\n",
    "shuffle_train = 1\n",
    "\n",
    "# Boolean that selects whether a data augmentation technique for DNNs is used\n",
    "data_augmentation = 0\n",
    "\n",
    "# Boolean that selects whether we start a new hyperparameter optimization or we restart an existing one\n",
    "new_hyperopt = 1\n",
    "\n",
    "# Number of years used in the training dataset for recalibration\n",
    "calibration_window = 4\n",
    "\n",
    "# Unique identifier to read the trials file of hyperparameter optimization\n",
    "experiment_id = 1\n",
    "\n",
    "# Number of iterations for hyperparameter optimization\n",
    "max_evals = 1500\n",
    "\n",
    "path_datasets_folder = \"./datasets/\"\n",
    "path_hyperparameters_folder = \"./experimental_files/\"\n",
    "\n",
    "# Check documentation of the hyperparameter_optimizer for each of the function parameters\n",
    "# In this example, we optimize a model for the PJM market.\n",
    "# We consider two directories, one for storing the datasets and the other one for the experimental files.\n",
    "# We start a hyperparameter optimization from scratch. We employ 1500 iterations in hyperopt,\n",
    "# 2 years of test data, a DNN with 2 hidden layers, a calibration window of 4 years,\n",
    "# we avoid data augmentation,  and we provide an experiment_id equal to 1\n",
    "hyperparameter_optimizer(path_datasets_folder=path_datasets_folder, \n",
    "                         path_hyperparameters_folder=path_hyperparameters_folder, \n",
    "                         new_hyperopt=new_hyperopt, max_evals=max_evals, nlayers=nlayers, dataset=dataset, \n",
    "                         years_test=years_test, calibration_window=calibration_window, \n",
    "                         shuffle_train=shuffle_train, data_augmentation=0, experiment_id=experiment_id,\n",
    "                         begin_test_date=begin_test_date, end_test_date=end_test_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test datasets: 2023-07-01 00:00:00 - 2023-07-07 23:00:00\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './experimental_files/DNN_hyperparameters_nl2_datDE_YT2_SF_CW4_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m path_recalibration_folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mexperimental_files\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     39\u001b[0m path_hyperparameter_folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mexperimental_files\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m evaluate_dnn_in_test_dataset(experiment_id, path_hyperparameter_folder\u001b[39m=\u001b[39;49mpath_hyperparameter_folder, \n\u001b[1;32m     42\u001b[0m                                path_datasets_folder\u001b[39m=\u001b[39;49mpath_datasets_folder, shuffle_train\u001b[39m=\u001b[39;49mshuffle_train, \n\u001b[1;32m     43\u001b[0m                                path_recalibration_folder\u001b[39m=\u001b[39;49mpath_recalibration_folder, \n\u001b[1;32m     44\u001b[0m                                nlayers\u001b[39m=\u001b[39;49mnlayers, dataset\u001b[39m=\u001b[39;49mdataset, years_test\u001b[39m=\u001b[39;49myears_test, \n\u001b[1;32m     45\u001b[0m                                data_augmentation\u001b[39m=\u001b[39;49mdata_augmentation, calibration_window\u001b[39m=\u001b[39;49mcalibration_window, \n\u001b[1;32m     46\u001b[0m                                new_recalibration\u001b[39m=\u001b[39;49mnew_recalibration, begin_test_date\u001b[39m=\u001b[39;49mbegin_test_date, \n\u001b[1;32m     47\u001b[0m                                end_test_date\u001b[39m=\u001b[39;49mend_test_date)\n",
      "File \u001b[0;32m~/git/test/epftoolbox/epftoolbox/models/_dnn.py:725\u001b[0m, in \u001b[0;36mevaluate_dnn_in_test_dataset\u001b[0;34m(experiment_id, path_datasets_folder, path_hyperparameter_folder, path_recalibration_folder, nlayers, dataset, years_test, shuffle_train, data_augmentation, calibration_window, new_recalibration, begin_test_date, end_test_date)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    723\u001b[0m     forecast_dates \u001b[39m=\u001b[39m forecast\u001b[39m.\u001b[39mindex\n\u001b[0;32m--> 725\u001b[0m model \u001b[39m=\u001b[39m DNN(experiment_id\u001b[39m=\u001b[39;49mexperiment_id, path_hyperparameter_folder\u001b[39m=\u001b[39;49mpath_hyperparameter_folder,\n\u001b[1;32m    726\u001b[0m             nlayers\u001b[39m=\u001b[39;49mnlayers, dataset\u001b[39m=\u001b[39;49mdataset, years_test\u001b[39m=\u001b[39;49myears_test, \n\u001b[1;32m    727\u001b[0m             shuffle_train\u001b[39m=\u001b[39;49mshuffle_train, data_augmentation\u001b[39m=\u001b[39;49mdata_augmentation, \n\u001b[1;32m    728\u001b[0m             calibration_window\u001b[39m=\u001b[39;49mcalibration_window)\n\u001b[1;32m    731\u001b[0m \u001b[39m# For loop over the recalibration dates\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[39mfor\u001b[39;00m date \u001b[39min\u001b[39;00m forecast_dates:\n\u001b[1;32m    733\u001b[0m \n\u001b[1;32m    734\u001b[0m     \u001b[39m# For simulation purposes, we assume that the available data is\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[39m# the data up to current date where the prices of current date are not known\u001b[39;00m\n",
      "File \u001b[0;32m~/git/test/epftoolbox/epftoolbox/models/_dnn.py:410\u001b[0m, in \u001b[0;36mDNN.__init__\u001b[0;34m(self, experiment_id, path_hyperparameter_folder, nlayers, dataset, years_test, shuffle_train, data_augmentation, calibration_window)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m dataset\n\u001b[1;32m    409\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalibration_window \u001b[39m=\u001b[39m calibration_window\n\u001b[0;32m--> 410\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_best_hyperapameters()\n",
      "File \u001b[0;32m~/git/test/epftoolbox/epftoolbox/models/_dnn.py:428\u001b[0m, in \u001b[0;36mDNN._read_best_hyperapameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m trials_file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_hyperparameter_folder, trials_file_name)\n\u001b[1;32m    427\u001b[0m \u001b[39m# Reading and extracting the best hyperparameters\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m trials \u001b[39m=\u001b[39m pc\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39;49m(trials_file_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    430\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_hyperparameters \u001b[39m=\u001b[39m format_best_trial(trials\u001b[39m.\u001b[39mbest_trial)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './experimental_files/DNN_hyperparameters_nl2_datDE_YT2_SF_CW4_2'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of layers in DNN\n",
    "nlayers = 2\n",
    "\n",
    "# Market under study. If it not one of the standard ones, the file name\n",
    "# has to be provided, where the file has to be a csv file\n",
    "dataset = 'DE'\n",
    "\n",
    "# Number of years (a year is 364 days) in the test dataset.\n",
    "years_test = 2\n",
    "\n",
    "# Boolean that selects whether the validation and training datasets were shuffled when\n",
    "# performing the hyperparameter optimization. Note that it does not select whether\n",
    "# shuffling is used for recalibration as for recalibration the validation and the\n",
    "# training datasets are always shuffled.\n",
    "shuffle_train = 1\n",
    "\n",
    "# Boolean that selects whether a data augmentation technique for DNNs is used\n",
    "data_augmentation = 0\n",
    "\n",
    "# Boolean that selects whether we start a new recalibration or we restart an existing one\n",
    "new_recalibration = 1\n",
    "\n",
    "# Number of years used in the training dataset for recalibration\n",
    "calibration_window = 4\n",
    "\n",
    "# Unique identifier to read the trials file of hyperparameter optimization\n",
    "experiment_id = 2\n",
    "\n",
    "# Optional parameters for selecting the test dataset, if either of them is not provided, \n",
    "# the test dataset is built using the years_test parameter. They should either be one of\n",
    "# the date formats existing in python or a string with the following format\n",
    "# \"%d/%m/%Y %H:%M\"\n",
    "begin_test_date = '01/07/2023'\n",
    "end_test_date = '07/07/2023'\n",
    "\n",
    "# Set up the paths for saving data (this are the defaults for the library)\n",
    "path_datasets_folder = os.path.join('.', 'datasets')\n",
    "path_recalibration_folder = os.path.join('.', 'experimental_files')\n",
    "path_hyperparameter_folder = os.path.join('.', 'experimental_files')\n",
    "\n",
    "evaluate_dnn_in_test_dataset(experiment_id, path_hyperparameter_folder=path_hyperparameter_folder, \n",
    "                               path_datasets_folder=path_datasets_folder, shuffle_train=shuffle_train, \n",
    "                               path_recalibration_folder=path_recalibration_folder, \n",
    "                               nlayers=nlayers, dataset=dataset, years_test=years_test, \n",
    "                               data_augmentation=data_augmentation, calibration_window=calibration_window, \n",
    "                               new_recalibration=new_recalibration, begin_test_date=begin_test_date, \n",
    "                               end_test_date=end_test_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
